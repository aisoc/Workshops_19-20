{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop (24/11/19) - Missing data imputation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/university-of-southampton-ai-society/notebooks/blob/master/Workshop_(24_11_19)_Missing_data_imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_KHaqmd6JX3",
        "colab_type": "text"
      },
      "source": [
        "# **Some background info**\n",
        "### **Types of missingness**\n",
        "There are 3 types of data missingness: MCAR (Missing Completely at Random), MAR (Missing at Random), and NMAR(Not Missing at Random). More specifically:\n",
        "\n",
        "\n",
        "1.   **MCAR** - missing value in the dataset is completely random. It is not correlated with features inside the dataset at all. Think of it as missingness that can be generated with typical random function, e.g. *np.rand.int()* \n",
        "2.   **MAR** - here the missingness is somewhat related to values that we can observe in the dataset. \n",
        "\n",
        "3. **NMAR** Missingness is related to the value that is missing or missing value is dependent on some other variable’s value. For example, imagine a simple dataset with two features salary and job title. Then it is possible that salary (Feature 1) might be sometimes missing if a person is a CEO, or CTO. The reason behind that might be that they tend to earn more than the average person and they don't feel comfortable disclosing their wage.\n",
        "\n",
        "Most of the imputation techniques work decently with MAR and MCAR but will fail badly with NMAR.\n",
        "Have look here at more information about these 3 types of randomness: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n",
        "\n",
        "### **Few useful imputation algorithms**\n",
        "\n",
        "*   **Removing the instance with missing values** - it is not really an imputation technique, but it might be a good alternative if there are not that many instance in the dataset with missing values.\n",
        "\n",
        "*  **Mean/Mode imputation** - Quite popular approach. For numerical features, impute the missing value with a mean, and for categorical ones impute it with the mode (i.e. the most frequent label in the feature). The drawback of this method is that it might hurt the model quite badly as they ignore the fact that some features can be correlated (and you can use that information to make more accurate imputations), and it also reduces the variance in the data which might bias the model and skew the results. In this article I ellaborate on it a bit more: https://towardsdatascience.com/why-using-a-mean-for-missing-data-is-a-bad-idea-alternative-imputation-algorithms-837c731c1008?source=friends_link&sk=429f2f54105cf63a4df61de693ea144b\n",
        "\n",
        "\n",
        "*   **K-NN imputation** - This imputation technique uses K-NearestNeighbors algorithm to impute the data. Basically, for a missing value in a Feature F1, the algorithm searches for K nearest neighbors (the \"closeness\" of the neighbor is based on some metric e.g. Squared difference). Once it has found K nearest neighbors that don't have Feature F1 missing, they compute the mean (mode for categorical variables) based on the K neighbors and replace it with a missing value. It is much better than a plain mean/mode imputation as it doesn't ignore feature correlations. It requires more computation power and time, though.\n",
        "\n",
        "* **MICE** - Consider it as a framework-like algorithm for missing data imputations. It works by filling the missing data multiple times with some underlying algorithm, e.g. LinearRegression or SVM. More info here: https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n",
        "\n",
        "* **MissForest** - uses RandomForest as an underlying algorithm to impute data. Good think about that is it innately handle both categorical and continuous features. Actually, you can say that MissForest is a MICE with a RandomForest Regression and RandomForest Classifier as underlying algorithms for missing data imputation. \n",
        "\n",
        "* **Honorable mentions** - there is much more different imputation  algorithms, each with different pros and cons. Quite interesting work has been done around Deep learning architectures for missing data imputations, e.g. GANs: https://towardsdatascience.com/gans-and-missing-data-imputation-815a0cbc4ece?source=friends_link&sk=3c6170ab92c564d6d77fc558879c1b35\n",
        "# **Some problems that I've encountered**\n",
        "Often, data is heterogeneous (i.e. it contains both continous feautures like temperature, height and categorical features like gender, or age). Lot of imputation libraries that are available online fail pretty badly if the dataset is heterogeneous. For some reason a lot of people just assumed that data is either continous or categorical. I wonder why...\n",
        "\n",
        "# **Plan of action**\n",
        "We will test how accurate imputations are for different imputation algorithms.\n",
        "More specifically, we will take a following approach for evaluating the algorithm:\n",
        "*    Load dataset (Iris) and randomly remove part of the data (MCAR)\n",
        "*    Save the removed data to compare it with imputed data later on\n",
        "*    Use an imputation algorithm to impute data\n",
        "*    Compute the loss between the true value and imputed value in the dataset\n",
        "\n",
        "# **Some imputation libraries in Python**\n",
        "*    **fancyimpute**\n",
        "*    **impyute**\n",
        "*     **missingpy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yatwBsnh5P0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4bf5442a-f420-4235-9193-be44aeb9b5c1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "from sklearn.datasets import load_iris\n",
        "from copy import deepcopy #to create a copy of the dataset\n",
        "\n",
        "!pip install impyute\n",
        "!pip install missingpy\n",
        "import impyute as imp # we'll use it for KNN, and MICE\n",
        "import missingpy as miss # we'll use it for MissForest"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: impyute in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from impyute) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from impyute) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from impyute) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->impyute) (0.14.0)\n",
            "Collecting missingpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/be/998d04d27054b58f0974b5f09f8457778a0a72d4355e0b7ae877b6cfb850/missingpy-0.2.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hInstalling collected packages: missingpy\n",
            "Successfully installed missingpy-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeu2-dMOFdt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load iris data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "# we will use only X because y is categorical and there are\n",
        "# not that many libraries supporting heteregeneous metrics\n",
        "# Create a copy of X to validate with imputations later on\n",
        "X_copy = deepcopy(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLgr9n8bGAih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to introduce randomnness to dataset\n",
        "def introduce_randomness(data, missingness_threshold):\n",
        "  row_cnt, col_cnt = data.shape\n",
        "  threshold = missingness_threshold * 100\n",
        "  for i in range(row_cnt):\n",
        "    for j in range(col_cnt):\n",
        "        rand_val = np.random.randint(100, size=1)\n",
        "        if rand_val >= threshold:\n",
        "            data[i, j] = np.nan\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKR9gs5uGoYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_missing = introduce_randomness(X, 0.5) # 50 % of data should be missing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuajKm2HMBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dde3cf3b-d52a-4a90-f82a-36a7a6e763d2"
      },
      "source": [
        "# Let's print it to see if it works\n",
        "print(X_missing[:10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 nan 1.4 nan]\n",
            " [nan 3.  nan 0.2]\n",
            " [nan nan nan nan]\n",
            " [4.6 3.1 nan nan]\n",
            " [nan 3.6 1.4 nan]\n",
            " [5.4 nan nan nan]\n",
            " [4.6 nan 1.4 nan]\n",
            " [5.  nan 1.5 nan]\n",
            " [4.4 nan nan nan]\n",
            " [nan 3.1 nan 0.1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok12C4KiKU62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_imputed = imp.mean(deepcopy(X_missing)) #Imputing with mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnCMtQ6rKoB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "7aa44909-4a41-4704-e0ac-ea6bf01e9151"
      },
      "source": [
        "# Let's have a look at quality of imputations\n",
        "print(f\"Imputed data: \\n{X_imputed[:10]}\\n\")\n",
        "# Let's have a look at statistical properties of true and imputed data\n",
        "from scipy import stats\n",
        "print(f\"Imputed data: \\n{stats.describe(X_imputed)}\\n\")\n",
        "print(f\"True data: \\n{stats.describe(X_copy)}\\n\")\n",
        "\n",
        "# We can also measure an error between imputed and true values.\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "for i in range (X_copy.shape[1]):\n",
        "  print(f\"MAE for a feature {i}: {mean_absolute_error(X_copy[:, i], X_imputed[:, i]):.2f}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imputed data: \n",
            "[[5.1        3.06029412 1.4        1.25769231]\n",
            " [5.84347826 3.         3.75492958 0.2       ]\n",
            " [5.84347826 3.06029412 3.75492958 1.25769231]\n",
            " [4.6        3.1        3.75492958 1.25769231]\n",
            " [5.84347826 3.6        1.4        1.25769231]\n",
            " [5.4        3.06029412 3.75492958 1.25769231]\n",
            " [4.6        3.06029412 1.4        1.25769231]\n",
            " [5.         3.06029412 1.5        1.25769231]\n",
            " [4.4        3.06029412 3.75492958 1.25769231]\n",
            " [5.84347826 3.1        3.75492958 0.1       ]]\n",
            "\n",
            "Imputed data: \n",
            "DescribeResult(nobs=150, minmax=(array([4.4, 2. , 1. , 0.1]), array([7.7, 4.4, 6.7, 2.5])), mean=array([5.84347826, 3.06029412, 3.75492958, 1.25769231]), variance=array([0.33805077, 0.09424694, 1.42681728, 0.28624419]), skewness=array([ 0.39592721,  0.38960195, -0.48072615, -0.35491982]), kurtosis=array([1.92947112, 4.28455599, 0.41128983, 0.41339875]))\n",
            "\n",
            "True data: \n",
            "DescribeResult(nobs=150, minmax=(array([4.3, 2. , 1. , 0.1]), array([7.9, 4.4, 6.9, 2.5])), mean=array([5.84333333, 3.05733333, 3.758     , 1.19933333]), variance=array([0.68569351, 0.18997942, 3.11627785, 0.58100626]), skewness=array([ 0.31175306,  0.31576711, -0.27212767, -0.10193421]), kurtosis=array([-0.57356795,  0.18097632, -1.39553589, -1.33606741]))\n",
            "\n",
            "MAE for a feature 0: 0.36\n",
            "MAE for a feature 1: 0.18\n",
            "MAE for a feature 2: 0.84\n",
            "MAE for a feature 3: 0.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjcgTqZwOe0q",
        "colab_type": "text"
      },
      "source": [
        "# Your Turn\n",
        "**Have a look at the different imputation algorithms from impyute and missingpy and try imputing the data as above. Check how MAE changes, together with statistical properties, e.g. mean and variance.**"
      ]
    }
  ]
}